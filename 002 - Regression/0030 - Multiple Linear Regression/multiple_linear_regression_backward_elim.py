# -*- coding: utf-8 -*-
"""Multiple_linear_regression_Backward_Elim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hgq8tA7QDVBfBlNKnR5Lc98XQqFiRaEc

# Multiple Linear Regression

## Importing the libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""## Importing the dataset"""

dataset = pd.read_csv('50_Startups.csv')

X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

print(X)

print(y)

"""## Encoding categorical data"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')
X = np.array(ct.fit_transform(X))

print(X)

"""### ** **In Multiple Linear Regression there is no need to apply feature scaling.**

## Avoiding the Dummy Variable Trap
"""

# Here I had manually removed the first coulmn to avoid DVT
# As here I'm doing Backward Elimination

# In general we don't have to remove manually a DVT because
# Scikit-Learn takes care of it.

X = X[:, 1:]

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)

"""## Training the Multiple Linear Regression model on the Training set"""

from sklearn.linear_model import LinearRegression

regressor = LinearRegression()

regressor.fit(X_train, y_train)

"""## Predicting the Test set results"""

y_pred = regressor.predict(X_test)

np.set_printoptions(precision=2)

print('   Predict    Real')
print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))

"""## Building the optimal model using Backward Elimination"""

import statsmodels.api as sm

# Creating a column with values of each row as 1 and adding
# it to the X
X = np.append(arr=np.ones((50, 1)).astype(int), values=X, axis=1)

print(X)

# X_opt will contain all the Independent Variables
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()

regressor_OLS.summary()

# x2(i.e., Index 2) has the highest P-value
# and we need to remove that and fit model again without this variable

X_opt = X[:, [0, 1, 3, 4, 5]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# x1(i.e., Index 1) has the highest P-value
# and we need to remove that and fit model again without this variable

X_opt = X[:, [0, 3, 4, 5]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# x2(i.e., Index 4) has the highest P-value
# and we need to remove that and fit model again without this variable

X_opt = X[:, [0, 3, 5]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# x2(i.e., Index 5) has the highest P-value
# and we need to remove that and fit model again without this variable

X_opt = X[:, [0, 3]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

"""From this we can see that the variable that can predict the highest profit with the statistical impact is the index 3 of X i.e., R&D Spend"""